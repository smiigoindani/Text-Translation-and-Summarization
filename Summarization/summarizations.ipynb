{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\n\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n\n\ndef textrank(document):\n    s_tokenizer = PunktSentenceTokenizer()\n    sentences = s_tokenizer.tokenize(document)\n\n    # build a Bag-of-words matrix\n    bow_matrix = CountVectorizer().fit_transform(sentences)\n    normalized = TfidfTransformer().fit_transform(bow_matrix)\n\n    similarity_graph = normalized * normalized.T\n\n    # from similarity graph, apply PageRank and sort the results based on rank\n    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n    scores = nx.pagerank(nx_graph)\n    sentence_array = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n\n    sentence_array = np.asarray(sentence_array)\n\n    fmax = float(sentence_array[0][0])\n    fmin = float(sentence_array[len(sentence_array) - 1][0])\n\n    temp_array = []\n    # Normalization\n    for i in range(0, len(sentence_array)):\n        if fmax - fmin == 0:\n            temp_array.append(0)\n        else:\n            temp_array.append((float(sentence_array[i][0]) - fmin) / (fmax - fmin))\n\n    # threshold to select only those sentences having more rank\n    threshold = (sum(temp_array) / len(temp_array)) + 0.2\n\n    sentence_list = []\n\n    for i in range(0, len(temp_array)):\n        if temp_array[i] > threshold:\n            sentence_list.append(sentence_array[i][1])\n\n    seq_list = []\n    for sentence in sentences:\n        if sentence in sentence_list:\n            seq_list.append(sentence)\n\n    return seq_list","metadata":{"execution":{"iopub.status.busy":"2022-08-15T01:01:19.662491Z","iopub.execute_input":"2022-08-15T01:01:19.663132Z","iopub.status.idle":"2022-08-15T01:01:21.843070Z","shell.execute_reply.started":"2022-08-15T01:01:19.662988Z","shell.execute_reply":"2022-08-15T01:01:21.842372Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install sumy","metadata":{"execution":{"iopub.status.busy":"2022-08-15T01:01:25.347645Z","iopub.execute_input":"2022-08-15T01:01:25.349802Z","iopub.status.idle":"2022-08-15T01:01:40.490402Z","shell.execute_reply.started":"2022-08-15T01:01:25.349750Z","shell.execute_reply":"2022-08-15T01:01:40.489271Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lex_rank import LexRankSummarizer\nfrom sumy.summarizers.lsa import LsaSummarizer\nimport pandas as pd\n\nsummaries1 = [] # for TextRank\nsummaries2 = [] # for LexRank\nsummaries3 = [] # for LSA\nsummarizer = LexRankSummarizer() # creating lexrank object()\nsumlsa = LsaSummarizer() # creatiing LSA object()\n\ndf = pd.read_csv('../input/bbcfull/BBC News Train.csv')\nfor _, row in df.iterrows():\n    data = row['Text']\n    \n    # TextRank approach\n    summ1 = textrank(data)\n    summaries1.append(summ1)\n    \n    parser = PlaintextParser.from_string(data, Tokenizer(\"english\"))\n\n\n    # LexRank approach\n    sum_lexrank = summarizer(parser.document, 3) #Summarizing the document\n    summ2 = ''\n    for s in sum_lexrank:\n        summ2 += ' ' + str(s)\n\n    cleaned_summ2 = summ2.strip()\n    summaries2.append(cleaned_summ2)\n    \n    # LSA approach\n    sum_lsa = sumlsa(parser.document, 3) #Summarizing the document\n    summ3 = ''\n    for s in sum_lsa:\n        summ3 += ' ' + str(s)\n\n    cleaned_summ3 = summ3.strip()\n    summaries3.append(cleaned_summ3)\n    \n    \ndf.insert(2, 'Summary1', summaries1)\ndf.insert(2, 'Summary2', summaries2)\ndf.insert(2, 'Summary3', summaries3)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T01:03:15.869193Z","iopub.execute_input":"2022-08-15T01:03:15.869561Z","iopub.status.idle":"2022-08-15T01:05:33.798215Z","shell.execute_reply.started":"2022-08-15T01:03:15.869506Z","shell.execute_reply":"2022-08-15T01:05:33.797392Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# save the dataframe to a CSV\ndf.to_csv(r'./output_summary3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:39:21.930965Z","iopub.execute_input":"2022-08-13T20:39:21.931426Z","iopub.status.idle":"2022-08-13T20:39:22.114020Z","shell.execute_reply.started":"2022-08-13T20:39:21.931387Z","shell.execute_reply":"2022-08-13T20:39:22.112992Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2022-08-15T01:05:54.286343Z","iopub.execute_input":"2022-08-15T01:05:54.287280Z","iopub.status.idle":"2022-08-15T01:06:04.771261Z","shell.execute_reply.started":"2022-08-15T01:05:54.287241Z","shell.execute_reply":"2022-08-15T01:06:04.770021Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom rouge_score import rouge_scorer\n\n# useing ROUGE-1 and ROUGE-L scores\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n\ndf = pd.read_csv('../input/ddptext/output_summary3.csv')\ndf = df.head(10)\nr1_1 = 0.0\nrl_1 = 0.0\nr1_1_1 = 0.0\nrl_1_1 = 0.0\nr1_1_2 = 0.0\nrl_1_2 = 0.0\nr1_2 = 0.0\nrl_2 = 0.0\nr1_2_1 = 0.0\nrl_2_1 = 0.0\nr1_2_2 = 0.0\nrl_2_2 = 0.0\nr1_3 = 0.0\nrl_3 = 0.0\nr1_3_1 = 0.0\nrl_3_1 = 0.0\nr1_3_2 = 0.0\nrl_3_2 = 0.0\n\nfor _, row in df.iterrows():\n    summary1 = row['Summary1']\n    summary2 = row['Summary2']\n    summary3 = row['Summary3']\n    \n    ref1 = row['REF1']\n    ref2 = row['REF2']\n    \n    scores = scorer.score(ref1 + ' ' + ref2 , summary1)\n    r1_1 += scores.get('rouge1').precision\n    rl_1 += scores.get('rougeL').precision\n    r1_1_1 += scores.get('rouge1').recall\n    rl_1_1 += scores.get('rougeL').recall\n    r1_1_2 += scores.get('rouge1').fmeasure\n    rl_1_2 += scores.get('rougeL').fmeasure\n    \n    scores = scorer.score(ref1 + ' ' + ref2 , summary2)\n    r1_2 += scores.get('rouge1').precision\n    rl_2 += scores.get('rougeL').precision\n    r1_2_1 += scores.get('rouge1').recall\n    rl_2_1 += scores.get('rougeL').recall\n    r1_2_2 += scores.get('rouge1').fmeasure\n    rl_2_2 += scores.get('rougeL').fmeasure\n    \n    scores = scorer.score(ref1 + ' ' + ref2 , summary3)\n    r1_3 += scores.get('rouge1').precision\n    rl_3 += scores.get('rougeL').precision\n    r1_3_1 += scores.get('rouge1').recall\n    rl_3_1 += scores.get('rougeL').recall\n    r1_3_2 += scores.get('rouge1').fmeasure\n    rl_3_2 += scores.get('rougeL').fmeasure\n\nprint('Rouge-1 (TextRank) [ precision', \"{:.2f}\".format(r1_1/10), ', recall', \"{:.2f}\".format(r1_1_1/10), ', fmeasure', \"{:.2f}\".format(r1_1_2/10),\"]\")\nprint('Rouge-L (TextRank) [ precision', \"{:.2f}\".format(rl_1/10), ', recall', \"{:.2f}\".format(rl_1_1/10), ', fmeasure', \"{:.2f}\".format(rl_1_2/10),\"]\")\nprint('Rouge-1 (LexRank)  [ precision', \"{:.2f}\".format(r1_2/10), ', recall', \"{:.2f}\".format(r1_2_1/10), ', fmeasure', \"{:.2f}\".format(r1_2_2/10),\"]\")\nprint('Rouge-L (LexRank)  [ precision', \"{:.2f}\".format(rl_2/10), ', recall', \"{:.2f}\".format(rl_2_1/10), ', fmeasure', \"{:.2f}\".format(rl_2_2/10),\"]\")\nprint('Rouge-1 (LSA)      [ precision', \"{:.2f}\".format(r1_3/10), ', recall', \"{:.2f}\".format(r1_3_1/10), ', fmeasure', \"{:.2f}\".format(r1_3_2/10),\"]\")\nprint('Rouge-L (LSA)      [ precision', \"{:.2f}\".format(rl_3/10), ', recall', \"{:.2f}\".format(rl_3_1/10), ', fmeasure', \"{:.2f}\".format(rl_3_2/10),\"]\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T01:06:07.580052Z","iopub.execute_input":"2022-08-15T01:06:07.580479Z","iopub.status.idle":"2022-08-15T01:06:08.270876Z","shell.execute_reply.started":"2022-08-15T01:06:07.580432Z","shell.execute_reply":"2022-08-15T01:06:08.269626Z"},"trusted":true},"execution_count":5,"outputs":[]}]}